{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Classification Models V2\n",
    "\n",
    "A series of modifications are made to the classification models in this notebook. The potential improvements being tested are:\n",
    "\n",
    "- Inclusion of bi and trigrams\n",
    "- Inclusion of POS tagged tokens\n",
    "- More model types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit learn n-grams**\n",
    "- CountVectorizer with ngrams 1 - n\n",
    "- Tf-idf\n",
    "- SVD (sklearn/gensime/jakevdp method)\n",
    "\n",
    "**Scikit learn n-grams**\n",
    "- CountVectorizer with ngrams 1 - n\n",
    "- Convert to gensim corpus\n",
    "- SVD gensim\n",
    "\n",
    "**Gensim n-grams**\n",
    "- Build grams up to n\n",
    "- Add n-grams onto original document by concatenating lists and dropping terms already in text\n",
    "\n",
    "**Separate ngram corpora**\n",
    "- Create separate bi and trigram corpora with gensim\n",
    "- ML on all three and combine\n",
    "\n",
    "** Split text into all possibel n-grams**\n",
    "- Use ALL ngrams\n",
    "- (is this what sklearn does anyway?)\n",
    "\n",
    "** Count Vectorizer **\n",
    "- Same as methods above but with no Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2 as pg\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from gensim.sklearn_integration.sklearn_wrapper_gensim_lsimodel import SklLsiModel\n",
    "from gensim.sklearn_integration.sklearn_wrapper_gensim_ldamodel import SklLdaModel\n",
    "import pycountry\n",
    "import unicodedata\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from idetect.geotagger import strip_accents, compare_strings, strip_words, common_names, LocationType\n",
    "from idetect.geotagger import subdivision_country_code, match_country_name, city_subdivision_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "- Import training data\n",
    "- Filter to English only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/stop_words_en_long.txt', 'r') as f:\n",
    "    stop_words = f.read()\n",
    "stop_words = stop_words.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Example Texts\n",
    "\n",
    "For quick testing purposes, a single example sentence, and a list of example sentences are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example_text = 'Flooding has stranded many people. 100 families fled across the border.'\n",
    "example_text = ['At least 6171 people in London were forced to evacuate homes, including more than 2600 staying in emergency shelters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentences = [u'Flooding has stranded many peole100 families were evacuated by the army.',\n",
    "                     u'Some people have fled to safety from the wild fire.',\n",
    "                     u'Flooding has stranded more people in nearby London.',\n",
    "                     u'It was all going ok but now flooding has stranded them.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence = \"\"\"At least six thousand people were forced to evacuate homes, including more than 2600 staying in emergency shelters, according to Taiwan's Central Emergency Operation Center. Taiwanese weather authorities lifted sea and land warnings on Thursday as Trami blew away from the island. Tell us more at jon@disasters.com.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(example_sentence)\n",
    "docs = [nlp(sent) for sent in example_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Processor\n",
    "\n",
    "The Cleaner class replaces commonly enountered errors in the texts that have been manually identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocationProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that replaces all country and subdivisions\n",
    "    mentioned in text with common names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tag_entities(self, text):\n",
    "        tokens = []\n",
    "        for token in text:\n",
    "            if token.ent_type_ == 'GPE':\n",
    "                if match_country_name(token.text)[0]:\n",
    "                    tokens.append('Switzerland')\n",
    "                elif city_subdivision_country(token.text):\n",
    "                    tokens.append('Zurich')\n",
    "                else:\n",
    "                    tokens.append('Zurich')\n",
    "            elif token.like_num:\n",
    "                tokens.append('1000')\n",
    "            elif token.like_url:\n",
    "                continue\n",
    "            elif token.like_email:\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(token.text)\n",
    "        return tokens\n",
    "\n",
    "    def join_phrases(self, phrases):\n",
    "        joined = []\n",
    "        for phrase in phrases:\n",
    "            tokens = []\n",
    "            for token in phrase:\n",
    "                if isinstance(token, spacy.tokens.token.Token):\n",
    "                    tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            joined.append('_'.join(tokens))\n",
    "        return joined\n",
    "    \n",
    "    def single_string(self, texts):\n",
    "        strings = [' '.join(t) for t in texts]\n",
    "        return strings\n",
    "    \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        texts = [nlp(t) for t in texts]\n",
    "        texts = [self.tag_entities(t) for t in texts]\n",
    "        texts = self.single_string(texts)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaner = LocationProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner.fit_transform(example_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Processor\n",
    "\n",
    "There is often significant vocabulary overlap between articles of different categories. However, the phrases that describe actual events are often different. The `Phraser` attempts to extract small snippets of phrases from the text in the hope that they may be common within a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhraseProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that turns documents in string form\n",
    "    into token lists, with various processing steps applied.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_tags : bool, required\n",
    "        Whether to tag words with their part of speech labels.\n",
    "    lemmatize : bool, required\n",
    "        Whether to lemmatize tokens.\n",
    "    stop_words : book, required\n",
    "        Whether to remove stop words.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    \n",
    "    def parse_phrases(self, doc):\n",
    "        '''Return a list of lists, with each sublist containing a token from the text,\n",
    "        it's parent token and it's grandparent token. Does not return any repeat tokens\n",
    "        in each phrase.'''\n",
    "        phrases = []\n",
    "        for d in doc:\n",
    "            if not d.is_punct:\n",
    "                if d.head != d:\n",
    "                    if d.head.head != d.head:\n",
    "                        phrases.append([d, d.head, d.head.head])\n",
    "                    else:\n",
    "                        phrases.append([d, d.head])\n",
    "        return phrases\n",
    "\n",
    "    def join_phrases(self, phrases):\n",
    "        joined = []\n",
    "        for phrase in phrases:\n",
    "            tokens = []\n",
    "            for token in phrase:\n",
    "                if isinstance(token, spacy.tokens.token.Token):\n",
    "                    tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            joined.append('_'.join(tokens))\n",
    "        return joined\n",
    "    \n",
    "    def single_string(self, texts):\n",
    "        strings = [' '.join(t) for t in texts]\n",
    "        return strings\n",
    "    \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        docs = [nlp(t) for t in texts]\n",
    "        phrases = [self.parse_phrases(d) for d in docs]\n",
    "        joined = [self.join_phrases(p) for p in phrases]\n",
    "        text = self.single_string(joined)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phraser = PhraseProcessor(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_sentences = phraser.transform(example_sentences)\n",
    "processed_sentence = phraser.transform(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tokenizer\n",
    "\n",
    "This processor has options to label the tokens of a text with their part of speech (POS) tags, lemmatize them, and remove stop words. It also removes certain POS entities and words of length < 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that turns documents in string form\n",
    "    into token lists, with various processing steps applied.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_tags : bool, required\n",
    "        Whether to tag words with their part of speech labels.\n",
    "    lemmatize : bool, required\n",
    "        Whether to lemmatize tokens.\n",
    "    stop_words : book, required\n",
    "        Whether to remove stop words.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words, pos_tags=True,\n",
    "                rejoin=True):\n",
    "        self.stop_words = stop_words\n",
    "        self.pos_tags = pos_tags\n",
    "        self.rejoin = rejoin\n",
    "\n",
    "    def tag_pos(self, text):\n",
    "        return [(t, t.pos_) for t in text]\n",
    "    \n",
    "    def get_lemmas(self, text):\n",
    "        return [t[0].lemma_ for t in text]\n",
    "\n",
    "    def remove_noise(self, text):\n",
    "        noise_tags = ['DET', 'NUM', 'SYM']\n",
    "        text = [t for t in text if t[0].text not in self.stop_words]\n",
    "        text = [t for t in text if len(t[0]) > 2]\n",
    "        text = [t for t in text if t[1] not in noise_tags]\n",
    "        text = [t for t in text if ~t[0].like_num]\n",
    "        return text\n",
    "    \n",
    "    def join_pos_lemmas(self, pos, lemmas):\n",
    "        return ['{}_{}'.format(l, p[1]).lower() for p, l\n",
    "                in zip(pos, lemmas)]\n",
    "    \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def single_string(self, texts):\n",
    "        strings = [' '.join(t) for t in texts]\n",
    "        return strings\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        docs = [nlp(sent) for sent in texts]\n",
    "        docs = [self.tag_pos(d) for d in docs]\n",
    "        docs = [self.remove_noise(d) for d in docs]\n",
    "        lemmas = [self.get_lemmas(d) for d in docs]\n",
    "        if self.pos_tags:\n",
    "            docs = [self.join_pos_lemmas(d, l) for d, l\n",
    "                    in zip(docs, lemmas)]\n",
    "        if self.rejoin:\n",
    "            docs = self.single_string(docs)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_processor = POSProcessor(stop_words, rejoin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_processor.fit_transform(example_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Processor\n",
    "\n",
    "The n-gram processor identifies common word co-occurences of between 2 or 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that finds and returns common bi and tri-grams\n",
    "    that appear in a text.\n",
    "    \n",
    "    Parameters\n",
    "   ----------\n",
    "    bi_min_count : int, required\n",
    "        Minimum number of times a bi-gram must occur in the corpus\n",
    "        to be counted.\n",
    "    bi_threshold : int, required\n",
    "        Threshold value used to calculate threshold scoring for \n",
    "        bi-grams to be counted. See gensim docs for more details.\n",
    "    tri_min_count : int, required\n",
    "        Minimum number of times a tri-gram must occur in the corpus\n",
    "        to be counted.\n",
    "    tri_threshold : int, required\n",
    "        Threshold value used to calculate threshold scoring for \n",
    "        tri-grams to be counted. See gensim docs for more details.\n",
    "    mode : 'str', required (default = 'trigram)\n",
    "        If 'trigram+', then original text is returned with both bi \n",
    "        and tri-grams replacing corresponding tokens.\n",
    "        If 'bigram+', then original text is returned with only bi-\n",
    "        grams replacing corresponding tokens.\n",
    "        If 'everything', then full original text is returned with all\n",
    "        bi and tri-grams appended.\n",
    "        If 'bigram\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bi_min_count=5, bi_threshold=10,\n",
    "                 tri_min_count=5, tri_threshold=10,\n",
    "                 mode='trigram'):\n",
    "        self.pos_processor = POSProcessor(stop_words, rejoin=False)\n",
    "        self.bi_min_count = bi_min_count\n",
    "        self.bi_threshold = bi_threshold\n",
    "        self.tri_min_count = tri_min_count\n",
    "        self.tri_threshold = tri_threshold\n",
    "        self.mode = mode\n",
    "    \n",
    "    def fit(self, texts, *args):\n",
    "        #docs = self.pre_processor.transform(texts)\n",
    "        self.build_grammer(texts)\n",
    "        return self\n",
    "    \n",
    "    def build_grammer(self, texts):\n",
    "        \"\"\"Creates bi and tri-gram Phraser models.\n",
    "        \"\"\"\n",
    "        self.bigram = Phrases(texts, \n",
    "                          min_count=self.bi_min_count, \n",
    "                          threshold=self.bi_threshold)\n",
    "        self.bigrammer = Phraser(self.bigram)\n",
    "        if (self.mode=='trigram') | (self.mode=='trigram+') | (self.mode=='everything'):\n",
    "            self.trigram = Phrases(self.bigrammer[texts], \n",
    "                              min_count=self.tri_min_count, \n",
    "                              threshold=self.tri_threshold)\n",
    "            self.trigrammer = Phraser(self.trigram)\n",
    "\n",
    "    def make_grams(self, text):\n",
    "        \"\"\"Applies Phraser models. Returns text with bigrams or trigrams replacing\n",
    "        their corresponding tokens, or both bigrams and trigrams, plus all original tokens.\"\"\"\n",
    "        bigrams = self.bigrammer[text]\n",
    "        if self.mode=='bigram':\n",
    "            return bigrams\n",
    "        elif (self.mode=='trigram') | (self.mode=='everything'):\n",
    "            trigrams = self.trigrammer[bigrams]\n",
    "            if self.mode=='trigram':\n",
    "                return trigrams\n",
    "            elif self.mode=='everything':\n",
    "                trigrams = [t for t in trigrams if t not in bigrams]\n",
    "                all_grams = bigrams + trigrams\n",
    "                return all_grams\n",
    "    \n",
    "    def merge(self, text, ngrams):\n",
    "        \"\"\"Returns text where the original text is kept, and\n",
    "        all n-grams are appended.\"\"\"\n",
    "        grams_only = [ng for ng in ngrams if ng not in list(set(text))]\n",
    "        return text + grams_only\n",
    "    \n",
    "    def stringify(self, grams):\n",
    "        gram_strs = []\n",
    "        for g in grams:\n",
    "            gram_strs.append(u' '.join(g))\n",
    "        return gram_strs\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        texts = self.pos_processor.fit_transform(texts)\n",
    "        grams = [self.make_grams(d) for d in texts]\n",
    "        if (self.mode=='bigram') | (self.mode=='trigram'):\n",
    "            return self.stringify(grams)\n",
    "        elif self.mode=='everything':\n",
    "            grammed_text = [self.merge(d, ng) for d, ng\n",
    "                            in zip(docs, grams)]\n",
    "            return self.stringify(grammed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = NGramProcessor(bi_min_count=2, bi_threshold=1,\n",
    "                 tri_min_count=1, tri_threshold=1,\n",
    "                 mode='bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer.fit(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammed_text = grammer.transform(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from gensim import matutils, models\n",
    "\n",
    "class CustomSklLsiModel(SklLsiModel):\n",
    "    \"\"\"Gensim's Lsi model with sklearn wrapper, modified to handle sparse matrices \n",
    "    for both fit and transform. Makes the class compatible with sklearn's Tfidf and \n",
    "    Count vectorizers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def sparse_2_tupes(self, sparse):\n",
    "        \"\"\"Converts sparse matrix into manageable tuple format.\"\"\"\n",
    "        for t in t_skltfidf:\n",
    "            cx = t.tocoo()\n",
    "            tups = []\n",
    "            for i, j in zip(cx.col, cx.data):\n",
    "                tups.append((i, j))\n",
    "        return tups\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the model according to the given training data.\n",
    "        Calls gensim.models.LsiModel\n",
    "        \"\"\"\n",
    "        if sparse.issparse(X):\n",
    "            corpus = matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "        else:\n",
    "            corpus = X\n",
    "\n",
    "        self.gensim_model = models.LsiModel(corpus=corpus, num_topics=self.num_topics, id2word=self.id2word, chunksize=self.chunksize,\n",
    "            decay=self.decay, onepass=self.onepass, power_iters=self.power_iters, extra_samples=self.extra_samples)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        \"\"\"\n",
    "        Takes a list of documents as input ('docs').\n",
    "        Returns a matrix of topic distribution for the given document bow, where a_ij\n",
    "        indicates (topic_i, topic_probability_j).\n",
    "        The input `docs` should be in BOW format and can be a list of documents like : [ [(4, 1), (7, 1)], [(9, 1), (13, 1)], [(2, 1), (6, 1)] ]\n",
    "        or a single document like : [(4, 1), (7, 1)]\n",
    "        \"\"\"\n",
    "        if self.gensim_model is None:\n",
    "            raise NotFittedError(\"This model has not been fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n",
    "\n",
    "        # The input as array of array\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # check = lambda x: [x] if isinstance(x[0], tuple) else x\n",
    "        # docs = check(docs)\n",
    "        if sparse.issparse(docs):\n",
    "            docs = matutils.Sparse2Corpus(docs, documents_columns=False)\n",
    "        X = [[] for i in range(0, len(docs))];\n",
    "        for k,v in enumerate(docs):\n",
    "            doc_topics = self.gensim_model[v]\n",
    "            probs_docs = list(map(lambda x: x[1], doc_topics))\n",
    "            # Everything should be equal in length\n",
    "            if len(probs_docs) != self.num_topics:\n",
    "                probs_docs.extend([1e-12]*(self.num_topics - len(probs_docs)))\n",
    "            X[k] = probs_docs\n",
    "            probs_docs = []\n",
    "        return np.reshape(np.array(X), (len(docs), self.num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomSklLdaModel(SklLdaModel):\n",
    "    \"\"\"Gensim's Lsi model with sklearn wrapper, modified to handle sparse matrices \n",
    "    for both fit and transform. Makes the class compatible with sklearn's Tfidf and \n",
    "    Count vectorizers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def sparse_2_tupes(self, sparse):\n",
    "        \"\"\"Converts sparse matrix into manageable tuple format.\"\"\"\n",
    "        for t in t_skltfidf:\n",
    "            cx = t.tocoo()\n",
    "            tups = []\n",
    "            for i, j in zip(cx.col, cx.data):\n",
    "                tups.append((i, j))\n",
    "        return tups\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the model according to the given training data.\n",
    "        Calls gensim.models.LsiModel\n",
    "        \"\"\"\n",
    "        if sparse.issparse(X):\n",
    "            corpus = matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "        else:\n",
    "            corpus = X\n",
    "\n",
    "        self.gensim_model = models.LdaModel(corpus=corpus, num_topics=self.num_topics, id2word=self.id2word,\n",
    "            chunksize=self.chunksize, passes=self.passes, update_every=self.update_every,\n",
    "            alpha=self.alpha, eta=self.eta, decay=self.decay, offset=self.offset,\n",
    "            eval_every=self.eval_every, iterations=self.iterations,\n",
    "            gamma_threshold=self.gamma_threshold, minimum_probability=self.minimum_probability,\n",
    "            random_state=self.random_state)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        \"\"\"\n",
    "        Takes a list of documents as input ('docs').\n",
    "        Returns a matrix of topic distribution for the given document bow, where a_ij\n",
    "        indicates (topic_i, topic_probability_j).\n",
    "        The input `docs` should be in BOW format and can be a list of documents like : [ [(4, 1), (7, 1)], [(9, 1), (13, 1)], [(2, 1), (6, 1)] ]\n",
    "        or a single document like : [(4, 1), (7, 1)]\n",
    "        \"\"\"\n",
    "        if self.gensim_model is None:\n",
    "            raise NotFittedError(\"This model has not been fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n",
    "\n",
    "        # The input as array of array\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # check = lambda x: [x] if isinstance(x[0], tuple) else x\n",
    "        # docs = check(docs)\n",
    "        if sparse.issparse(docs):\n",
    "            docs = matutils.Sparse2Corpus(docs, documents_columns=False)\n",
    "        X = [[] for i in range(0, len(docs))];\n",
    "        for k, v in enumerate(docs):\n",
    "            doc_topics = self.gensim_model[v]\n",
    "            probs_docs = list(map(lambda x: x[1], doc_topics))\n",
    "            # Everything should be equal in length\n",
    "            if len(probs_docs) != self.num_topics:\n",
    "                probs_docs.extend([1e-12]*(self.num_topics - len(probs_docs)))\n",
    "            X[k] = probs_docs\n",
    "        return np.reshape(np.array(X), (len(docs), self.num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "## Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel = pd.read_csv('../data/training_data_clean_09242017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rel['is_displacement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'x'\n",
    "    \n",
    "def english_only(df):\n",
    "    language = df['text'].apply(detect_lang)\n",
    "    df = df[language == 'en']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = english_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rel['is_displacement_label'] = le.fit_transform(df_rel['is_displacement'])\n",
    "df_rel_modeling = df_rel.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaner = CleaningProcessor()\n",
    "# df_rel_modeling['text'] = cleaner.fit_transform(df_rel_modeling['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_rel_modeling.to_csv('../../data/training_data_clean_09242017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_rel_train, X_rel_test, y_rel_train, y_rel_test = train_test_split(df_rel_modeling['text'], \n",
    "                                                    df_rel_modeling['is_displacement_label'],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_processor = POSProcessor(stop_words)\n",
    "phrase_processor = PhraseProcessor(stop_words)\n",
    "pos_processor, phrase_processor = (pos_processor.fit(X_train), phrase_processor.fit(X_train))\n",
    "X_train_pos, X_train_phrase = (pos_processor.transform(X_train), phrase_processor.transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_pos, X_test_phrase = (pos_processor.transform(X_test), phrase_processor.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all = []\n",
    "for pos, phrase in zip(X_train_pos, X_train_phrase):\n",
    "    X_train_all.append(pos + phrase)\n",
    "\n",
    "X_test_all = []\n",
    "for pos, phrase in zip(X_test_pos, X_test_phrase):\n",
    "    X_test_all.append(pos + phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def splitter(x):\n",
    "#     s = x.split(' ')\n",
    "#     return s\n",
    "\n",
    "# word_counter = Counter()\n",
    "# for word in X_train.apply(splitter):\n",
    "#     word_counter.update(word)\n",
    "    \n",
    "# sorted_x = sorted(word_counter.items(), key=operator.itemgetter(1),\n",
    "#                   reverse=True)\n",
    "# len(sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Uncomment to see frequency of words in corpus\n",
    "\n",
    "# sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Vectorising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = ['leave', 'evacuate', 'homeless', 'forced', 'flee', 'fled', 'destroyed', 'destruction', \n",
    "         'submerged', 'wrecked', 'washed', 'away', 'devastated', 'under', 'water', 'underwater', \n",
    "         'inundated', 'camp', 'collapse', 'left', 'reconstruct', 'demolished', 'uninhabitable', \n",
    "         'border', 'across', 'refugee', 'shelter', 'crops', 'corn', 'rice', 'maize', 'wheat', \n",
    "         'field']\n",
    "counter = CountVectorizer(vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set number of Lsi topics here\n",
    "\n",
    "num_topics = 50\n",
    "\n",
    "processor = Pipeline([\n",
    "               ('tfidf', TfidfVectorizer(norm='l2')),\n",
    "               ('lsi', CustomSklLsiModel(num_topics=num_topics))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processor = processor.fit(X_train)\n",
    "processor_pos = processor.fit(X_train_pos)\n",
    "# processor_phrase = processor.fit(X_train_phrase)\n",
    "# processor_all = processor.fit(X_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lsi Vector Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_processor_vectors(X, y, processor):\n",
    "    vecs = processor.transform(X)\n",
    "    \n",
    "    v0 = []\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    v3 = []\n",
    "    v4 = []\n",
    "    for v in vecs:\n",
    "        v0.append(v[0])\n",
    "        v1.append(v[1])\n",
    "        v2.append(v[2])\n",
    "        v3.append(v[3])\n",
    "        v4.append(v[4])\n",
    "\n",
    "    df = pd.DataFrame(data={'v0': v0, 'v1': v1, 'v2': v2, 'v3':v3, 'v4': v4, 'category': y})\n",
    "            \n",
    "    sns.pairplot(df, hue=\"category\", plot_kws={'alpha': 0.2})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = visualize_processor_vectors(X_train_pos, y_train, processor_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Topic Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get vocabulary from tfidf to assess topics in LSI model\n",
    "tfidf = processor.named_steps['tfidf']\n",
    "id2word = {t[1]: t[0] for t in list(tfidf.vocabulary_.items())}\n",
    "lsi_model = processor.named_steps['lsi'].gensim_model\n",
    "lsi_model.id2word = id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Resolving Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models chosen are all capable of multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_jobs = -1\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_jobs=n_jobs)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=n_jobs)\n",
    "\n",
    "svm_clf = LinearSVC()\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Mode\n",
    "\n",
    "Simple mode means transforming all of the features once for all of the classifiers. This has the advantage of speed, but the disadvantage of not tuning the input features for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('../python/idetect/nlp_models/relevance_classifier_svm_10052017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.predict(['Lots of people were evacuated due to the flooding.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_pipe = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "        ('phrase', Pipeline([\n",
    "            ('processor', PhraseProcessor(stop_words)),\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000)),\n",
    "            ('lsi', CustomSklLsiModel(num_topics=300))\n",
    "        ])),\n",
    "        ('pos', Pipeline([\n",
    "            ('processor', POSProcessor(stop_words)),\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000, max_df=0.7)),\n",
    "            ('lsi', CustomSklLsiModel(num_topics=300))\n",
    "        ]))\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi_pipe = joblib.load(lsi_pipe, 'lsi_union_10022017.pkl')\n",
    "lsi_pipe.fit(df_rel_modeling['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lsi_vecs_rel_train = lsi_pipe.fit_transform(X_rel_train)\n",
    "# lsi_vecs_rel_test = lsi_pipe.transform(X_rel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_vecs_rel_all = lsi_pipe.transform(df_rel_modeling['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(lsi_pipe, 'lsi_union_10022017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = lsi_pipe.named_steps['union']\n",
    "tfidf_phrase = union.transformer_list[0][1].named_steps['tfidf']\n",
    "tfidf_pos = union.transformer_list[0][1].named_steps['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 'A', 'b': 'B'}\n",
    "for a,b in d.items():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_to_id2word(vocab):\n",
    "    id2word = []\n",
    "    for k, v in vocab.items():\n",
    "        id2word.append((v, k))\n",
    "    return id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# union.transformer_list[0][1].named_steps['lsi'].id2word = vocab_to_id2word(tfidf_phrase.vocabulary_)\n",
    "# union.transformer_list[1][1].named_steps['lsi'].id2word = vocab_to_id2word(tfidf_pos.vocabulary_)\n",
    "# union.transformer_list[0][1].named_steps['lsi'].gensim_model.id2word = vocab_to_id2word(tfidf_phrase.vocabulary_)\n",
    "# union.transformer_list[1][1].named_steps['lsi'].gensim_model.id2word = vocab_to_id2word(tfidf_pos.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_pos = union.transformer_list[1][1].named_steps['lsi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_phrase.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm_params = {'C': [0.03, 0.1, 0.3, 1, 3, 10]}\n",
    "svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm_grid.fit(lsi_vecs_rel_train, y_rel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_best = svm_grid.best_estimator_\n",
    "svm_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_preds = svm_best.predict(lsi_vecs_rel_test)\n",
    "print(classification_report(y_rel_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_final = LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_final.fit(lsi_vecs_rel_all, df_rel_modeling['is_displacement_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = Pipeline([\n",
    "    ('location', LocationProcessor()),\n",
    "    ('lsi', lsi_pipe),\n",
    "    ('svm', svm_final)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svm_pipe, 'relevance_classifier_svm_10132017.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_params = {'C': [0.03, 0.1, 0.3, 1, 3, 10]}\n",
    "lr_grid = GridSearchCV(lr_clf, lr_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid.fit(lsi_vecs_rel_train, y_rel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_best = lr_grid.best_estimator_\n",
    "lr_preds = lr_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_rel_test, lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnb_clf.fit(lsi_vecs_rel_train, y_rel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_best = gnb_clf\n",
    "gnb_preds = gnb_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_rel_test, lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_params = {'n_estimators': [1000],\n",
    "             'max_features': [18, 19, 20],\n",
    "             'max_depth': [100],\n",
    "             'min_samples_split': [4,5,6]}\n",
    "rf_grid = GridSearchCV(rf_clf, rf_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = rf_grid.best_estimator_\n",
    "rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors': [2,3,4,5,6,7],\n",
    "             'leaf_size': [10,20,30,40]}\n",
    "knn_grid = GridSearchCV(knn_clf, knn_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_grid.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best = knn_grid.best_estimator_\n",
    "knn_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds = knn_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, knn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [svm_best, gnb_best, lr_best]\n",
    "sclf = StackingClassifier(classifiers=clfs,\n",
    "                          meta_classifier=GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf.fit(lsi_vecs_train, y_train)\n",
    "# joblib.load(sclf, 'stacked_classifier_10022017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sclf.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sclf, 'stacked_classifier_10022017.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_pipe = Pipeline([\n",
    "    ('cleaner', CleaningProcessor()),\n",
    "    ('lsi', lsi_pipe),\n",
    "    ('stacked', sclf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pipe.fit(df_rel_modeling['text'], df_rel_modeling['is_displacement_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(stacked_pipe, 'relevance_classifier_10032017.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cat = pd.read_csv('../../data/category_training_data_en_09102017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cat_modeling = df_cat.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaner = CleaningProcessor()\n",
    "df_cat_modeling['text'] = cleaner.fit_transform(df_cat_modeling['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_cat_modeling['category_label'] = le.fit_transform(df_cat_modeling['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(df_cat_modeling['text'], \n",
    "                                                    df_cat_modeling['category_label'],\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_processor = POSProcessor(stop_words)\n",
    "phrase_processor = PhraseProcessor(stop_words)\n",
    "pos_processor, phrase_processor = (pos_processor.fit(X_train), phrase_processor.fit(X_train))\n",
    "X_train_pos, X_train_phrase = (pos_processor.transform(X_train), phrase_processor.transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_pos, X_test_phrase = (pos_processor.transform(X_test), phrase_processor.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_pipe = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "        ('phrase', Pipeline([\n",
    "            ('processor', PhraseProcessor(stop_words)),\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000)),\n",
    "            ('lda', CustomSklLsiModel(num_topics=300))\n",
    "        ])),\n",
    "        ('pos', Pipeline([\n",
    "            ('processor', POSProcessor()),\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000)),\n",
    "            ('lda', CustomSklLsiModel(num_topics=300))\n",
    "        ]))\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lsi_pipe = joblib.load(lsi_pipe, 'lsi_union_10022017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsi_vecs_train = lsi_pipe.fit_transform(X_cat_train)\n",
    "lsi_vecs_test = lsi_pipe.transform(X_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_jobs = -1\n",
    "\n",
    "rf_cat_clf = RandomForestClassifier(n_jobs=n_jobs)\n",
    "\n",
    "knn_cat_clf = KNeighborsClassifier(n_jobs=n_jobs)\n",
    "\n",
    "svm_cat_clf = LinearSVC(multi_class='ovr')\n",
    "\n",
    "gnb_cat_clf = GaussianNB()\n",
    "\n",
    "lr_cat_clf = LogisticRegression(multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_cat_params = {'C': [0.1, 0.9, 1, 1.1, 1.2]}\n",
    "svm_cat_grid = GridSearchCV(svm_cat_clf, svm_cat_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_cat_grid.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cat_best = svm_cat_grid.best_estimator_\n",
    "svm_cat_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cat_preds = svm_cat_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, svm_cat_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_cat_params = {'C': [8, 9, 10, 11, 12, 100]}\n",
    "lr_cat_grid = GridSearchCV(lr_cat_clf, lr_cat_params, cv=5, scoring=None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_cat_grid.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cat_best = lr_cat_grid.best_estimator_\n",
    "lr_cat_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_cat_preds = lr_cat_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, lr_cat_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnb_cat_clf.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_cat_best = gnb_cat_clf\n",
    "gnb_cat_preds = gnb_cat_best.predict(lsi_vecs_test)\n",
    "print(classification_report(y_test, gnb_cat_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = [svm_cat_best, gnb_cat_best, lr_cat_best]\n",
    "sclf_cat = StackingClassifier(classifiers=clfs,\n",
    "                          meta_classifier=LinearSVC(multi_class='ovr', C=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf_cat.fit(lsi_vecs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sclf_cat_preds = sclf_cat.predict(lsi_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, sclf_cat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sclf_cat_grid = GridSearchCV(sclf_cat, sclf_cat_params, cv=5, scoring=None, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf.fit(lsi_vecs_train, y_train)\n",
    "# joblib.load(sclf, 'stacked_classifier_10022017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('cleaner', CleaningProcessor()),\n",
    "    ('lsi', lsi_pipe),\n",
    "    ('svm', svm_cat_best)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline.fit(df_cat_modeling['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(cat_pipeline, 'category_classifier_10032017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
