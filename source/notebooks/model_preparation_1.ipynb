{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "import operator\n",
    "#import matplotlib\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/stop_words_en_long.txt', 'r') as f:\n",
    "    stop_words = f.read()\n",
    "stop_words = stop_words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/classification_training.csv')\n",
    "df.drop(['url', 'Unnamed: 0', 'Unnamed: 0.1', 'langs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_stems(text, stop_words):\n",
    "    tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [t for t in tokens if t not in stop_words] \n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    stems = [s.lower() for s in stems]\n",
    "    #stems = [s for s in stems if not s.isdigit()]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_tokens(text, stop_words):\n",
    "    tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [t for t in tokens if t not in stop_words] \n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df['text'], \n",
    "                                                                            df['category'], \n",
    "                                                                            test_size=0.2)\n",
    "# X_train_stems, X_test_stems, y_train_stems, y_test_stems = sklearn.model_selection.train_test_split(df['stems'], \n",
    "#                                                                                                     df['category'], \n",
    "#                                                                                                     test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Word Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword method is wrapped in an `scikit-learn` estimator. Although it does not need to be fitted, this allows us to call `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stemmer(TransformerMixin):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "    def prepare_stems(self, text, stop_words):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "        tokens = [t for t in tokens if t not in stop_words] \n",
    "        stems = [self.stemmer.stem(t) for t in tokens]\n",
    "        stems = [s.lower() for s in stems]\n",
    "        stems = [s for s in stems if not s.isdigit()]\n",
    "        return stems\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        X = X.map(lambda x: self.prepare_stems(x, self.stop_words))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer(TransformerMixin):\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        \n",
    "    def prepare_tokens(self, text, stop_words):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tokens = [t for t in tokens if not t.isdigit()]\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        X = X.map(lambda x: self.prepare_tokens(x, self.stop_words))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KeyWordClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.create_keywords()\n",
    "    \n",
    "    def create_keywords(self):\n",
    "        conflict_tokens = ['war', 'conflict', 'military', 'ceasefire', 'terrorism', 'fighting', 'militia', 'rebels', \n",
    "                  'violence', 'violent', 'clash', 'insurgent', 'besiege', 'bomb', 'gun', 'combat', 'siege',\n",
    "                  'battle', 'battleground', 'explode', 'explosive', 'peace', 'truce', 'airstrike', 'armed', 'weapon']\n",
    "        self.conflict_stems = [self.stemmer.stem(token) for token in conflict_tokens]\n",
    "        disaster_tokens = ['flood', 'wildfire', 'fire', 'earthquake', 'mudslide', 'landslide', 'washed', 'hurricane',\n",
    "                          'storm', 'rain', 'rainfall', 'river', 'sea', 'disaster', 'volcano', 'typhoon', 'blaze',\n",
    "                         'tremor', 'drought', 'disease', 'malnutrition', 'virus', 'health', 'tornado', 'forest', 'snow']\n",
    "        self.disaster_stems = [self.stemmer.stem(token) for token in disaster_tokens]\n",
    "        return self\n",
    "    \n",
    "    def tag_by_stem(self, texts, conflict_stems, disaster_stems):\n",
    "        equals = []\n",
    "        categories = []\n",
    "        tag_dicts = []\n",
    "        for text in texts:\n",
    "            tag_dictionary = {'conflict': 0, 'disaster': 0}\n",
    "            for stem in conflict_stems:\n",
    "                tag_dictionary['conflict'] = tag_dictionary['conflict'] + text.count(stem)\n",
    "            for stem in disaster_stems:\n",
    "                tag_dictionary['disaster'] = tag_dictionary['disaster'] + text.count(stem)\n",
    "\n",
    "            if tag_dictionary['conflict'] == 0 and tag_dictionary['disaster'] == 0:\n",
    "                category = 'other'\n",
    "                e = True\n",
    "            elif tag_dictionary['conflict'] == tag_dictionary['disaster']:\n",
    "                category = 'other'\n",
    "                e = True\n",
    "            else:\n",
    "                category = max(tag_dictionary, key=tag_dictionary.get)\n",
    "                e = False\n",
    "            categories.append(category)\n",
    "            tag_dicts.append(tag_dictionary)\n",
    "            equals.append(e)\n",
    "        return categories\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        y = self.tag_by_stem(X, self.conflict_stems, self.disaster_stems)\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        y = self.tag_by_stem(X, self.conflict_stems, self.disaster_stems)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_pipeline = Pipeline([\n",
    "        ('stemmer', Stemmer(stop_words=stop_words)),\n",
    "        ('kw_clf', KeyWordClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = kw_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.80      0.68      0.74       149\n",
      "   disaster       0.91      0.91      0.91       357\n",
      "      other       0.60      0.68      0.64       144\n",
      "\n",
      "avg / total       0.81      0.81      0.81       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our machine learning process is tf-idf. This transformer wraps the `gensim` implementation in `sklearn`'s API.\n",
    "\n",
    "`fit` creates a dictionary, transforms the corpus using a bag of words approach, and then builds the tfidf model based on this. `transform` then applies this model to any collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfTransformer(TransformerMixin):\n",
    "    def __init__(self, no_below=5, no_above=0.5, tfidf_model=None, dictionary=None):\n",
    "        self.dictionary = dictionary\n",
    "        self.tfidf_model = tfidf_model\n",
    "        self.no_below = no_below\n",
    "        self.no_above = no_above\n",
    "    \n",
    "    def make_dictionary(self, texts):\n",
    "        if not self.dictionary:\n",
    "            self.dictionary = gensim.corpora.Dictionary(texts)\n",
    "            if self.no_below or self.no_above:\n",
    "                self.dictionary.filter_extremes(no_below=self.no_below, no_above=self.no_above)\n",
    "        return self\n",
    "    \n",
    "    def tfidf_transform(texts, dictionary=None, tfidf_model=None):\n",
    "        if not dictionary:\n",
    "            dictionary = gensim.corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        if not tfidf_model:\n",
    "            tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "        corpus_tfidf = tfidf_model[corpus]\n",
    "        return corpus_tfidf, dictionary, tfidf_model\n",
    "    \n",
    "    def make_corpus(self, texts):\n",
    "        corpus = [self.dictionary.doc2bow(text) for text in texts]\n",
    "        return corpus\n",
    "    \n",
    "    def make_tfidf_model(self, corpus):\n",
    "        if not self.tfidf_model:\n",
    "            self.tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, texts, y=None):\n",
    "        self.make_dictionary(texts)\n",
    "        self.corpus = self.make_corpus(texts)\n",
    "        self.make_tfidf_model(self.corpus)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        corpus = self.make_corpus(texts)\n",
    "        return self.tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LsiTransformer(TransformerMixin):\n",
    "    def __init__(self, n_dimensions=100, no_below=5, no_above=0.5, lsi_model=None):\n",
    "        self.lsi_model = lsi_model\n",
    "        self.n_dimensions = n_dimensions\n",
    "        self.no_below = no_below\n",
    "        self.no_above = no_above\n",
    "    \n",
    "    def build_tfidf(self, texts):\n",
    "        self.tfidf_transformer = TfidfTransformer(no_below=self.no_below, no_above=self.no_above)\n",
    "        self.tfidf_transformer.fit(texts)\n",
    "        corpus_tfidf = self.tfidf_transformer.transform(texts)\n",
    "        dictionary = self.tfidf_transformer.dictionary\n",
    "        return corpus_tfidf, dictionary\n",
    "    \n",
    "    def lsi_to_vecs(self, corpus_lsi):\n",
    "        lsi_vecs = []\n",
    "        for c in corpus_lsi:\n",
    "            vec = [x[1] for x in c]\n",
    "            lsi_vecs.append(vec)\n",
    "        return np.array(lsi_vecs)\n",
    "    \n",
    "    def make_lsi_model(self, texts):    \n",
    "        self.corpus_tfidf, self.dictionary = self.build_tfidf(texts)\n",
    "        if not self.lsi_model:\n",
    "            self.lsi_model = gensim.models.LsiModel(self.corpus_tfidf, \n",
    "                                                    id2word=self.dictionary, \n",
    "                                                    num_topics=self.n_dimensions)\n",
    "        return self\n",
    "    \n",
    "    def make_corpus(self, corpus_tfidf):\n",
    "        lsi_corpus = self.lsi_model[corpus_tfidf]\n",
    "        return lsi_corpus\n",
    "    \n",
    "    def fit(self, texts, *args, **kwargs):\n",
    "        self.make_lsi_model(texts)\n",
    "        self.corpus_lsi = self.lsi_model[self.corpus_tfidf]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        corpus_tfidf = self.tfidf_transformer.transform(texts)\n",
    "        corpus_lsi = self.make_corpus(corpus_tfidf)\n",
    "        #return sparse2full(corpus_lsi, self.n_dimensions)\n",
    "        return self.lsi_to_vecs(corpus_lsi)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tf-Idf + LSI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_pipe = Pipeline([\n",
    "        ('tokenizer', Tokenizer(stop_words=stop_words)),\n",
    "        ('lsi', LsiTransformer(no_below=2, no_above=0.1)),\n",
    "        ('model', RandomForestClassifier(max_features=15, n_estimators=1000, n_jobs=3))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tokenizer', <__main__.Tokenizer object at 0x7f4d778cfd68>), ('lsi', <__main__.LsiTransformer object at 0x7f4d778cfdd8>), ('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=15, max_leaf_nodes=None,\n",
       "            min_im...n_jobs=3,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     lsi_pipe.predict(pd.Series(['']))\n",
    "# except ValueError as e:\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.79      0.80      0.79       148\n",
      "   disaster       0.95      0.88      0.91       386\n",
      "      other       0.60      0.74      0.66       116\n",
      "\n",
      "avg / total       0.85      0.84      0.84       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(lsi_pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'].str.len().nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.drop([183, 151, 86, 260, 265], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsi_pipe.fit(df['text'], df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-084b064a42b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../idetect/source/python/relevance.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/George/miniconda3/envs/nlp3.6/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/George/miniconda3/envs/nlp3.6/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/Users/George/miniconda3/envs/nlp3.6/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 60"
     ]
    }
   ],
   "source": [
    "joblib.load('../idetect/source/python/relevance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lsi_pipe.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lsi_pipe, 'lsi_pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series('evacuated from the flood by the military')\n",
    "lsi_pipe.predict(a)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Approaches with Custom Rules Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Combiner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.kw = KeyWordClassifier()\n",
    "        self.lsi = lsi_pipe\n",
    "\n",
    "    def combine_predictions(self, classified, keyword_tagged):\n",
    "        predictions = []\n",
    "        for classifier, keyword in zip(classified, keyword_tagged):\n",
    "            if classifier == keyword:\n",
    "                predictions.append(keyword)\n",
    "            elif keyword == 'unknown':\n",
    "                predictions.append(classifier)\n",
    "            else:\n",
    "                predictions.append(classifier)\n",
    "        return predictions\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.lsi.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, *args):\n",
    "        self.classified = self.lsi.predict(X)\n",
    "        self.keyword_tagged = self.kw.transform(X)\n",
    "        return pd.DataFrame({'classified': self.classified, 'key_word': self.keyword_tagged})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiLabelEncoder(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(X.iloc[:,0])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        columns = X.columns\n",
    "        for column in columns:\n",
    "            X[column] = self.le.transform(X[column])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-146d0a212cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-7af32acdb5ac>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "m = MultiLabelEncoder()\n",
    "m.fit(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Combiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combiner()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = c.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.34      0.76      0.47        75\n",
      "   disaster       0.64      0.86      0.73       256\n",
      "      other       0.73      0.32      0.44       319\n",
      "\n",
      "avg / total       0.65      0.58      0.56       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions['key_word'], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classified</th>\n",
       "      <th>key_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conflict</td>\n",
       "      <td>conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conflict</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disaster</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classified  key_word\n",
       "0  conflict   conflict\n",
       "1  conflict   other   \n",
       "2  other      other   \n",
       "3  disaster   other   \n",
       "4  other      other   "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.87      0.83      0.85       181\n",
      "   disaster       0.89      0.95      0.92       337\n",
      "      other       0.70      0.61      0.65       132\n",
      "\n",
      "avg / total       0.85      0.85      0.85       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.87      0.83      0.85       181\n",
      "   disaster       0.89      0.95      0.92       337\n",
      "      other       0.70      0.61      0.65       132\n",
      "\n",
      "avg / total       0.85      0.85      0.85       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, c.classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   conflict       0.86      0.72      0.79       181\n",
      "   disaster       0.89      0.89      0.89       337\n",
      "      other       0.57      0.65      0.61       132\n",
      "    unknown       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.82      0.80      0.80       650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/George/miniconda3/envs/nlp3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, c.keyword_tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class T1(TransformerMixin):\n",
    "    \n",
    "    def fit(self, Xy=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X1 = X\n",
    "        X2 = X*2\n",
    "        return X1, X2\n",
    "    \n",
    "class T2(TransformerMixin):\n",
    "    \n",
    "    def fit(self, X1, X2, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X1, X2, y=None):\n",
    "        y = X1 + X2\n",
    "        return y\n",
    "    \n",
    "    def fit_transform(self, X1, X2, y=None):\n",
    "        return self.fit(X1, X2).transform(X1, X2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = T1()\n",
    "x1, x2 = t1.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  6,  9, 12, 15])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = T2()\n",
    "t2.fit_transform(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "        ('t1', T1()),\n",
    "        ('t2', T2())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('t1', <__main__.T1 object at 0x11c747dd8>), ('t2', <__main__.T2 object at 0x11c747da0>)])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Approaches with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vtg = VotingClassifier(estimators=[('kw', kw_clf), ('lsi', lsi_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('kw', KeyWordClassifier()), ('lsi', Pipeline(steps=[('tfidf', <__main__.LsiTransformer object at 0x12a28c208>), ('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_imp...ors=1000, n_jobs=3, oob_score=False, random_state=123,\n",
       "            verbose=0, warm_start=False))]))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = vtg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_transform(texts, dictionary=None, tfidf_model=None):\n",
    "    if not dictionary:\n",
    "        dictionary = gensim.corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    if not tfidf_model:\n",
    "        tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf_model[corpus]\n",
    "    return corpus_tfidf, dictionary, tfidf_model\n",
    "\n",
    "def lsi_transform(corpus_tfidf, dictionary, lsi_model=None, dimensions=2):\n",
    "    if not lsi_model:\n",
    "        lsi_model = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=dimensions)\n",
    "    corpus_lsi = lsi_model[corpus_tfidf]\n",
    "    lsi_vecs = lsi_to_vecs(corpus_lsi)\n",
    "    return lsi_vecs, corpus_lsi, lsi_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_relevance = pd.read_csv('../../../data/relevance_training.csv')\n",
    "df_relevance.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RelevanceKeyWordClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.create_keywords()\n",
    "    \n",
    "    def create_keywords(self):\n",
    "        displacement_tokens = ['evacuated', 'evacuee', 'displaced', 'displacement', 'fled', 'stranded', 'homeless', \n",
    "                  'flee', 'rescued', 'trapped', 'shelter', 'camp', 'escape', 'forced', 'migrant', 'run', 'ran']\n",
    "        self.displacement_stems = [self.stemmer.stem(token) for token in displacement_tokens]\n",
    "        return self\n",
    "    \n",
    "    def tag_by_stem(self, texts, displacement_stems):\n",
    "        is_displacement = []\n",
    "        for text in texts:\n",
    "            mentions = 0\n",
    "            for stem in self.displacement_stems:\n",
    "                mentions += text.count(stem)\n",
    "            if mentions > 0:\n",
    "                is_displacement.append('yes')\n",
    "            else:\n",
    "                is_displacement.append('no')\n",
    "        return is_displacement\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        y = self.tag_by_stem(X, self.displacement_stems)\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        y = self.tag_by_stem(X, self.displacement_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_kw_pipeline = Pipeline([\n",
    "        ('stemmer', Stemmer(stop_words=stop_words)),\n",
    "        ('tagger', RelevanceKeyWordClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = rel_kw_pipeline.fit_transform(df_relevance['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.92      0.75      0.82      1927\n",
      "        yes       0.71      0.90      0.79      1313\n",
      "\n",
      "avg / total       0.83      0.81      0.81      3240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_relevance['is_displacement'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_ml_pipeline = Pipeline([\n",
    "        ('tokenizer', Tokenizer(stop_words=stop_words)),\n",
    "        ('lsi', LsiTransformer(no_below=2, no_above=0.1)),\n",
    "        ('model', RandomForestClassifier(max_features=12, n_estimators=1000, n_jobs=3))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rel, test_rel = sklearn.model_selection.train_test_split(df_relevance, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_rel = train_rel['is_displacement']\n",
    "test_labels_rel = test_rel['is_displacement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tokenizer', <__main__.Tokenizer object at 0x119c71940>), ('lsi', <__main__.LsiTransformer object at 0x119c71d68>), ('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=12, max_leaf_nodes=None,\n",
       "            min_impurity...ators=1000, n_jobs=3, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ml_pipeline.fit(train_rel['text'], train_labels_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = rel_ml_pipeline.predict(test_rel['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.73      0.90      0.80       388\n",
      "        yes       0.77      0.49      0.60       260\n",
      "\n",
      "avg / total       0.74      0.74      0.72       648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels_rel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [{'max_features': [7, 8, 9, 10, 11, 12],\n",
    "               'n_jobs': [3],\n",
    "              }]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=3)\n",
    "clf = model_selection.GridSearchCV(rf, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword and Random Forest Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Combiner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ml_model, kw_model):\n",
    "        self.ml_model = ml_model\n",
    "        self.kw_model = kw_model\n",
    "        \n",
    "    def combine_relevance_tags(self, classified, keyword_tagged):\n",
    "        combined = []\n",
    "        for classifier, keyword in zip(classified, keyword_tagged):\n",
    "            if keyword == 'no' and classifier == 'no':\n",
    "                tag = 'no'\n",
    "            elif keyword == 'yes' and classifier == 'yes':\n",
    "                tag = 'yes'\n",
    "            elif keyword == 'no' and classifier == 'yes':\n",
    "                tag = 'yes'\n",
    "            elif keyword == 'yes' and classifier == 'no':\n",
    "                tag = 'yes'\n",
    "            combined.append(tag)\n",
    "        return combined\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, *args):\n",
    "        ml_tagged = self.ml_model.predict(X)\n",
    "        kw_tagged = self.kw_model.transform(X)\n",
    "        combined = self.combine_relevance_tags(ml_tagged, kw_tagged)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Combiner(rel_ml_pipeline, rel_kw_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relevance_classifier.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(c, 'relevance_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_predictions = c.transform(test_rel['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.68      0.95      0.80       278\n",
      "        yes       0.95      0.67      0.78       370\n",
      "\n",
      "avg / total       0.84      0.79      0.79       648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(combined_predictions, test_labels_rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on IDMC Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_idmc_test = pd.read_csv('../../data/test/all_scraped_test_urls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoryModel(object):\n",
    "    def __init__(self, model_path=None):\n",
    "        self.model = self.load_model(model_path=model_path)\n",
    "\n",
    "    def load_model(self, model_path=None):\n",
    "        if model_path and os.path.isfile(model_path):\n",
    "            clf = joblib.load(model_path)\n",
    "        else:\n",
    "            default_model_path = 'category.pkl'\n",
    "            if os.path.isfile(default_model_path):\n",
    "                clf = joblib.load(default_model_path)\n",
    "            else:\n",
    "                url = 'https://s3-us-west-2.amazonaws.com/idmc-idetect/category_models/category.pkl'\n",
    "                r = requests.get(url, stream=True)\n",
    "                if not os.path.isfile(default_model_path):\n",
    "                    try:\n",
    "                        os.makedirs(os.path.dirname(default_model_path))\n",
    "                    except OSError as exc: # Guard against race condition\n",
    "                        if exc.errno != errno.EEXIST:\n",
    "                            raise\n",
    "                with open(default_model_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024):\n",
    "                        if chunk:  # filter out keep-alive new chunks\n",
    "                            f.write(chunk)\n",
    "                clf = joblib.load(default_model_path)\n",
    "        return clf\n",
    "\n",
    "    def predict(self, text):\n",
    "        try:\n",
    "            category = self.model.predict(pd.Series(text))[0]\n",
    "        except:\n",
    "            # if error occurs, classify as most likely category\n",
    "            category = 'disaster'\n",
    "\n",
    "        if category == 'disaster':\n",
    "            return 'disaster'\n",
    "            #return Category.DISASTER\n",
    "        elif category == 'conflict':\n",
    "            return 'conflict'\n",
    "            #return Category.CONFLICT\n",
    "        else:\n",
    "            return 'other'\n",
    "            #return Category.OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
